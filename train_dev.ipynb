{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as sched\n",
    "import torch.utils.data as data\n",
    "import util\n",
    "\n",
    "from args import get_train_args\n",
    "from collections import OrderedDict\n",
    "from json import dumps\n",
    "from models import BiDAF\n",
    "#from tensorboardX import SummaryWriter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm import tqdm\n",
    "from ujson import load as json_load\n",
    "from util import collate_fn, SQuAD\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "argDict = {'train_record_file': './data/train.npz', \n",
    "        'dev_record_file': './data/dev.npz', \n",
    "        'test_record_file': './data/test.npz', \n",
    "        'word_emb_file': './data/word_emb.json', \n",
    "        'char_emb_file': './data/char_emb.json', \n",
    "        'train_eval_file': './data/train_eval.json', \n",
    "        'dev_eval_file': './data/dev_eval.json', \n",
    "        'test_eval_file': './data/test_eval.json', \n",
    "        'name': 'devNonPCE', \n",
    "        'max_ans_len': 15, \n",
    "        'num_workers': 4, \n",
    "        'save_dir': './save/', \n",
    "        'batch_size': 16, \n",
    "        'use_squad_v2': True, \n",
    "        'hidden_size': 100, \n",
    "        'num_visuals': 10, \n",
    "        'load_path': None, \n",
    "        'rnn_type': 'LSTM', \n",
    "        'char_embeddings': False, \n",
    "        'eval_steps': 5000, \n",
    "        'lr': 0.5, 'l2_wd': 0, \n",
    "        'num_epochs': 3, \n",
    "        'drop_prob': 0.2, \n",
    "        'metric_name': 'F1', \n",
    "        'max_checkpoints': 5, \n",
    "        'max_grad_norm': 5.0, \n",
    "        'seed': 224, \n",
    "        'ema_decay': 0.999, \n",
    "        'char_out_channels': 5, \n",
    "        'char_kernel_size': 100, \n",
    "        'maximize_metric': True,\n",
    "        'gpu_ids': []}\n",
    "args = Namespace(**argDict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06.12.21 14:24:39] Args: {\n",
      "    \"batch_size\": 16,\n",
      "    \"char_emb_file\": \"./data/char_emb.json\",\n",
      "    \"char_embeddings\": false,\n",
      "    \"char_kernel_size\": 100,\n",
      "    \"char_out_channels\": 5,\n",
      "    \"dev_eval_file\": \"./data/dev_eval.json\",\n",
      "    \"dev_record_file\": \"./data/dev.npz\",\n",
      "    \"drop_prob\": 0.2,\n",
      "    \"ema_decay\": 0.999,\n",
      "    \"eval_steps\": 5000,\n",
      "    \"gpu_ids\": [\n",
      "        0\n",
      "    ],\n",
      "    \"hidden_size\": 100,\n",
      "    \"l2_wd\": 0,\n",
      "    \"load_path\": null,\n",
      "    \"lr\": 0.5,\n",
      "    \"max_ans_len\": 15,\n",
      "    \"max_checkpoints\": 5,\n",
      "    \"max_grad_norm\": 5.0,\n",
      "    \"maximize_metric\": true,\n",
      "    \"metric_name\": \"F1\",\n",
      "    \"name\": \"devNonPCE\",\n",
      "    \"num_epochs\": 3,\n",
      "    \"num_visuals\": 10,\n",
      "    \"num_workers\": 4,\n",
      "    \"rnn_type\": \"LSTM\",\n",
      "    \"save_dir\": \"./save/train\\\\devNonPCE-01\",\n",
      "    \"seed\": 224,\n",
      "    \"test_eval_file\": \"./data/test_eval.json\",\n",
      "    \"test_record_file\": \"./data/test.npz\",\n",
      "    \"train_eval_file\": \"./data/train_eval.json\",\n",
      "    \"train_record_file\": \"./data/train.npz\",\n",
      "    \"use_squad_v2\": true,\n",
      "    \"word_emb_file\": \"./data/word_emb.json\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "    # Set up logging and devices\n",
    "    args.save_dir = util.get_save_dir(args.save_dir, args.name, training=True)\n",
    "    log = util.get_logger(args.save_dir, args.name)\n",
    "    tbx = SummaryWriter(args.save_dir)\n",
    "    #if args.device_cpu:\n",
    "    #    device = 'cpu',\n",
    "    #    args.gpu_ids = []\n",
    "    #else:\n",
    "    device, args.gpu_ids = util.get_available_devices()\n",
    "    log.info(f'Args: {dumps(vars(args), indent=4, sort_keys=True)}')\n",
    "    args.batch_size *= max(1, len(args.gpu_ids))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(batch_size=16, char_emb_file='./data/char_emb.json', char_embeddings=False, char_kernel_size=100, char_out_channels=5, dev_eval_file='./data/dev_eval.json', dev_record_file='./data/dev.npz', drop_prob=0.2, ema_decay=0.999, eval_steps=5000, gpu_ids=[0], hidden_size=100, l2_wd=0, load_path=None, lr=0.5, max_ans_len=15, max_checkpoints=5, max_grad_norm=5.0, maximize_metric=True, metric_name='F1', name='devNonPCE', num_epochs=3, num_visuals=10, num_workers=4, rnn_type='LSTM', save_dir='./save/train\\\\devNonPCE-01', seed=224, test_eval_file='./data/test_eval.json', test_record_file='./data/test.npz', train_eval_file='./data/train_eval.json', train_record_file='./data/train.npz', use_squad_v2=True, word_emb_file='./data/word_emb.json')\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.is_available()\n",
    "print(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06.12.21 14:24:55] Using random seed 224...\n"
     ]
    }
   ],
   "source": [
    "    # Set random seed\n",
    "    log.info(f'Using random seed {args.seed}...')\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06.12.21 14:25:04] Loading embeddings...\n"
     ]
    }
   ],
   "source": [
    "    # Get embeddings\n",
    "    log.info('Loading embeddings...')\n",
    "    word_vectors = util.torch_from_json(args.word_emb_file)\n",
    "    char_vectors = util.torch_from_json(args.char_emb_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06.12.21 14:59:58] Building model...\n"
     ]
    }
   ],
   "source": [
    "    # Get model\n",
    "    log.info('Building model...')\n",
    "    model = BiDAF(word_vectors = word_vectors,\n",
    "                  char_vectors = char_vectors,\n",
    "                  hidden_size=args.hidden_size,\n",
    "                  rnn_type=args.rnn_type,\n",
    "                  drop_prob=args.drop_prob)\n",
    "\n",
    "    #if args.device_cpu:\n",
    "    #    args.gpu_ids = []\n",
    "    #    device = 'cpu'\n",
    "    #else:\n",
    "    #model = nn.DataParallel(model, args.gpu_ids)\n",
    "    if args.load_path:\n",
    "        log.info(f'Loading checkpoint from {args.load_path}...')\n",
    "        model, step = util.load_model(model, args.load_path, args.gpu_ids)\n",
    "    else:\n",
    "        step = 0\n",
    "    model = model.to(device)\n",
    "    model.train()\n",
    "    ema = util.EMA(model, args.ema_decay)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiDAF(\n",
      "  (emb): Embedding(\n",
      "    (word_embed): Embedding(88714, 300)\n",
      "    (proj): Linear(in_features=300, out_features=100, bias=False)\n",
      "    (char_embed): Embedding(1376, 64)\n",
      "    (char_cnn): Conv1d(64, 100, kernel_size=(5,), stride=(1,))\n",
      "    (hwy): HighwayEncoder(\n",
      "      (transforms): ModuleList(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      )\n",
      "      (gates): ModuleList(\n",
      "        (0): Linear(in_features=200, out_features=200, bias=True)\n",
      "        (1): Linear(in_features=200, out_features=200, bias=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (enc): RNNEncoder(\n",
      "    (rnn): LSTM(200, 100, num_layers=2, batch_first=True, dropout=0.2, bidirectional=True)\n",
      "  )\n",
      "  (att): BiDAFAttention()\n",
      "  (mod): RNNEncoder(\n",
      "    (rnn): LSTM(800, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (att2): BiDAFAttention()\n",
      "  (mod2): RNNEncoder(\n",
      "    (rnn): LSTM(600, 100, batch_first=True, bidirectional=True)\n",
      "  )\n",
      "  (out): BiDAFOutput(\n",
      "    (att_linear_1): Linear(in_features=200, out_features=1, bias=True)\n",
      "    (mod_linear_1): Linear(in_features=200, out_features=1, bias=True)\n",
      "    (rnn): RNNEncoder(\n",
      "      (rnn): LSTM(200, 100, batch_first=True, bidirectional=True)\n",
      "    )\n",
      "    (att_linear_2): Linear(in_features=200, out_features=1, bias=True)\n",
      "    (mod_linear_2): Linear(in_features=200, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model)\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06.12.21 14:54:32] Saver will maximize F1...\n"
     ]
    }
   ],
   "source": [
    "    # Get saver\n",
    "    saver = util.CheckpointSaver(args.save_dir,\n",
    "                                 max_checkpoints=args.max_checkpoints,\n",
    "                                 metric_name=args.metric_name,\n",
    "                                 maximize_metric=args.maximize_metric,\n",
    "                                 log=log)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Get optimizer and scheduler\n",
    "    optimizer = optim.Adadelta(model.parameters(), args.lr,\n",
    "                               weight_decay=args.l2_wd)\n",
    "    scheduler = sched.LambdaLR(optimizer, lambda s: 1.)  # Constant LR\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[06.12.21 14:54:35] Building dataset...\n"
     ]
    }
   ],
   "source": [
    "    # Get data loader\n",
    "    log.info('Building dataset...')\n",
    "    train_dataset = SQuAD(args.train_record_file, args.use_squad_v2)\n",
    "    train_loader = data.DataLoader(train_dataset,\n",
    "                                   batch_size=args.batch_size,\n",
    "                                   shuffle=True,\n",
    "                                   num_workers=args.num_workers,\n",
    "                                   collate_fn=collate_fn)\n",
    "    dev_dataset = SQuAD(args.dev_record_file, args.use_squad_v2)\n",
    "    dev_loader = data.DataLoader(dev_dataset,\n",
    "                                 batch_size=args.batch_size,\n",
    "                                 shuffle=False,\n",
    "                                 num_workers=args.num_workers,\n",
    "                                 collate_fn=collate_fn)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset', 'num_workers', 'prefetch_factor', 'pin_memory', 'timeout', 'worker_init_fn', '_DataLoader__multiprocessing_context', '_dataset_kind', 'batch_size', 'drop_last', 'sampler', 'batch_sampler', 'generator', 'collate_fn', 'persistent_workers', '_DataLoader__initialized', '_IterableDataset_len_called', '_iterator', '__module__', '__annotations__', '__doc__', '__init__', '_get_iterator', 'multiprocessing_context', '__setattr__', '__iter__', '_auto_collation', '_index_sampler', '__len__', 'check_worker_number_rationality', '__orig_bases__', '__dict__', '__weakref__', '__parameters__', '__slots__', '_is_protocol', '__new__', '__class_getitem__', '__init_subclass__', '__repr__', '__hash__', '__str__', '__getattribute__', '__delattr__', '__lt__', '__le__', '__eq__', '__ne__', '__gt__', '__ge__', '__reduce_ex__', '__reduce__', '__subclasshook__', '__format__', '__sizeof__', '__dir__', '__class__']\n",
      "<torch.utils.data.sampler.BatchSampler object at 0x0000021317A53D00>\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.__dir__())\n",
    "print(train_loader.batch_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # get a training input\n",
    "    cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids = next(iter(train_loader))\n",
    "    cw_idxs=cw_idxs.to(device)\n",
    "    cc_idxs=cc_idxs.to(device)\n",
    "    qw_idxs=qw_idxs.to(device)\n",
    "    qc_idxs=qc_idxs.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cw_idxs torch.Size([16, 204]) cuda:0\n",
      "cc_idxs torch.Size([16, 204, 16])\n",
      "qw_idxs torch.Size([16, 22])\n",
      "qc_idxs torch.Size([16, 22, 16])\n"
     ]
    }
   ],
   "source": [
    "print('cw_idxs',cw_idxs.shape,cw_idxs.device)\n",
    "print('cc_idxs',cc_idxs.shape)\n",
    "print('qw_idxs',qw_idxs.shape)\n",
    "print('qc_idxs',qc_idxs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "    tbx.add_graph(model,[cw_idxs, cc_idxs, qw_idxs, qc_idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    # Train\n",
    "    log.info('Training...')\n",
    "    steps_till_eval = args.eval_steps\n",
    "    epoch = step // len(train_dataset)\n",
    "    while epoch != args.num_epochs:\n",
    "        epoch += 1\n",
    "        log.info(f'Starting epoch {epoch}...')\n",
    "        with torch.enable_grad(), \\\n",
    "                tqdm(total=len(train_loader.dataset)) as progress_bar:\n",
    "            for cw_idxs, cc_idxs, qw_idxs, qc_idxs, y1, y2, ids in train_loader:\n",
    "                # Setup for forward\n",
    "                cw_idxs = cw_idxs.to(device)\n",
    "                qw_idxs = qw_idxs.to(device)\n",
    "                cc_idxs = cc_idxs.to(device)\n",
    "                qc_idxs = qc_idxs.to(device)\n",
    "                batch_size = cw_idxs.size(0)\n",
    "                optimizer.zero_grad()\n",
    "                \n",
    "                # Forward\n",
    "                log_p1, log_p2 = model(cw_idxs,cc_idxs, qw_idxs, qc_idxs)\n",
    "                y1, y2 = y1.to(device), y2.to(device)\n",
    "                loss = F.nll_loss(log_p1, y1) + F.nll_loss(log_p2, y2)\n",
    "                loss_val = loss.item()\n",
    "\n",
    "                # Backward\n",
    "                loss.backward()\n",
    "                nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step() # removed parm step // batch_size per scheduler 1.8 release notes\n",
    "                ema(model, step // batch_size)\n",
    "\n",
    "                # Log info\n",
    "                step += batch_size\n",
    "                progress_bar.update(batch_size)\n",
    "                progress_bar.set_postfix(epoch=epoch,\n",
    "                                         NLL=loss_val)\n",
    "\n",
    "                tbx.add_scalar('train/NLL', loss_val, step)\n",
    "                tbx.add_scalar('train/LR',\n",
    "                               optimizer.param_groups[0]['lr'],\n",
    "                               step)\n",
    "                \n",
    "                steps_till_eval -= batch_size\n",
    "                if steps_till_eval <= 0:\n",
    "                    steps_till_eval = args.eval_steps\n",
    "\n",
    "                    # Evaluate and save checkpoint\n",
    "                    log.info(f'Evaluating at step {step}...')\n",
    "                    ema.assign(model)\n",
    "                    results, pred_dict = evaluate(model, dev_loader, device,\n",
    "                                                  args.dev_eval_file,\n",
    "                                                  args.max_ans_len,\n",
    "                                                  args.use_squad_v2)\n",
    "                    saver.save(step, model, results[args.metric_name], device)\n",
    "                    ema.resume(model)\n",
    "\n",
    "                    # Log to console\n",
    "                    results_str = ', '.join(f'{k}: {v:05.2f}' for k, v in results.items())\n",
    "                    log.info(f'Dev {results_str}')\n",
    "\n",
    "                    # Log to TensorBoard\n",
    "                    log.info('Visualizing in TensorBoard...')\n",
    "                    for k, v in results.items():\n",
    "                        tbx.add_scalar(f'dev/{k}', v, step)\n",
    "                    util.visualize(tbx,\n",
    "                                   pred_dict=pred_dict,\n",
    "                                   eval_path=args.dev_eval_file,\n",
    "                                   step=step,\n",
    "                                   split='dev',\n",
    "                                   num_visuals=args.num_visuals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    hparms = args.__dict__.copy()\n",
    "    hparms['gpu_ids']=str(args.gpu_ids)\n",
    "    metrics =  dict([ ('met/'+k,v) for (k,v) in results.items()])\n",
    "    tbx.add_hparams(hparms,metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
